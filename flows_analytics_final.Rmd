---
title: "Credit Easing and ETF Flows"
author: 
  - Lisha Li
  - Barry Quinn
  - Lisa Sheenan
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    self_contained: false
    toc: true
    cold_folding: true
---

```{r set-up, include=FALSE}
knitr::opts_chunk$set(message = FALSE,warnings=FALSE, code_folding=TRUE)
dels<-ls()
rm(list=dels)
pacman::p_load("tidyverse","readxl","lubridate",'vars','corrplot','data.table')
read_excel("raw.xlsx",
           sheet = "daily values",
           skip = 3,
           col_types = "text") %>% 
  dplyr::slice(-c(1:2)) %>%
  drop_na(`BFFUEIG Index`) %>%
  rename(Date="...1") %>%
  mutate(datE=as.Date(Date,"%m/%d/%Y"),
         datE=if_else(is.na(datE),
                      as.Date(as.numeric(Date),
                              origin = "1899-12-30"),datE),Date=NULL)->dat
dat<-bind_cols(Date=dat$datE,dat %>% 
                 dplyr::select(-datE) %>% 
                 map_df(as.numeric))
# drop all missing variables
dat<-dat[-c(17:19)]
excel_sheets("raw.xlsx")->sheet_names
read_excel("raw.xlsx",sheet =sheet_names[1])[,1:2]->flow_names
new_names<-flow_names$Description[match(names(dat),flow_names$Security)]
new_names<-str_replace(new_names,"US ETF Flows - ","")
new_names[17:19]<-c("US 3M LIBOR","SONIA","EU 3M LIBOR")
names(dat)[-1]<-new_names[-1]

dat %>% 
  arrange(Date) %>%
  mutate(
    d_US=c(NA,diff(`US 3M LIBOR`)),
         d_SONIA=c(NA,diff(`SONIA`)),
         d_EU=c(NA,diff(`EU 3M LIBOR`)),
         CE_dummy=if_else(Date>=as.Date("23-3-2020","%d-%m-%Y") & 
                            Date<=as.Date("31-12-2020","%d-%m-%Y"),1,0)) %>%
  ungroup() %>%
  dplyr::select(-`Convertible Bonds`,
                -`US 3M LIBOR`,-`EU 3M LIBOR`,-`SONIA`
                ) %>%
  drop_na() ->y_endog
```

# Introduction

Quantitative easing (QE), basically the expansion of a central bank’s balance sheet by means of the large scale purchase of government bonds or other assets with the aim of stimulating the economy,originated in Japan in 2001.  QE focuses on bank reserves, or central bank liabilities.  The term credit easing (CE) was coined in 2009 by Ben Bernanke, the then Chairman of the Federal Reserve,and focuses on the asset side of the balance sheet with the aim of stimulating credit markets.These markets were hit hard during the global financial crisis of 2007-2008 and central banks like the Federal Reserve and the ECB began introducing credit support measures in its aftermath in an effort to aid recovery and trigger growth. These are part of a suite of unconventional monetary policy measures implemented by central banks in recent years.

On 31<sup>st</sup> January 2020 the World Health Organisation (WHO) declared a global health emergency due to the COVID-19 pandemic, leading to lockdowns across the globe and, with this, a slowdown in the global economy and need for large scale assistance measures.  On 23<sup>rd</sup> March 2020 the Federal Reserve announced the establishment of the Primary Market Corporate Credit Facility(PMCCF)  and  the  Secondary  Corporate  Credit  Facility  (SMCCF),  both  aimed  at  supporting corporate bond markets. The SMCCF involves the purchase of corporate bonds in the secondary market, as well as U.S.-listed exchange-traded funds (ETFs).The European Central Bank had introduced a similar credit support program in June 2016 with the Corporate Sector Purchase Program (CSPP), which was then expanded on 18th March 2020 to include non-financial commercial paper, thus making all qualifying commercial papers eligible for purchase under the program.Although ETFs are not eligible for purchase under the CSPP following the initial announcement(and prior to the release of full details) of the program on 10th March 2016 corporate bond ETF trading boomed.  The website ETFStrategy.com reported 

>Data from European-listed ETF trading platform Tradeweb showed that trading in fixed income ETFs increased to 50.3% as a proportion of overall traded volume. The majority of this, 62.9%, was in corporate and high yield bonds, with ‘buys’ in corporate bond ETFs nearly double the amount of ‘sells’.

We investigate the dynamic impact of fixed income ETF fund flows and central bank interest rates.  Specifically, we assume that there is a substantive non-linear structure to bivariate relationship which we explore in two-stages. In stage one we use assess the bivariate information flow between fixed income ETFs and central bank interest rates using transfer entropy, a model-free measure of the asymmetric information flow between time series in a network. This is a popular approach to information diffusion and contagion in a complex systems such as global capital markets[@Bekiros2017,@Nam2019].

This paper aims to assess the impact of these credit easing programs on the underlying corporate bonds  in  order  to  ascertain  if  the  inclusion  of  EFTs  could  have  an  effect  on  the  program’s performance. While a number of studies compare quantitative easing to credit easing (refs) this work is the first to date to compare the performance of credit easing programs across countries.

# Data
The data is sourced from Bloomberg and consists of daily indices capturing net ETF Flows into different bond categories of ETF in the US market. The data is for the period `r first(dat$Date)` - `r last(dat$Date)`.  The sample also include three interest ratios, US 3-month LIBOR, EU 3-Month LIBOR and the SONIA interest rate benchmark.

```{r plot}
y_endog %>% 
  dplyr::select(-Date,-CE_dummy) %>%
  map_df(scale) %>%
bind_cols(y_endog %>% 
            dplyr::select(Date,CE_dummy)) ->y_endog_scaled
y_endog_scaled %>%
  gather(variable,value,-Date) %>%
    mutate(predictors=if_else(variable %in% c('d_US','d_EU','d_SONIA'),"Rate_Change","Flows")) %>%
  ggplot(aes(x=Date,y=value,colour=variable)) +
  geom_line() +
  labs(title="Interests and ETF Flows") +
  facet_wrap(~variable,scales = "free_y") +
  theme(legend.position = 'none')

```


## Linear pairwise correlations

```{r correlation}
cor(y_endog_scaled %>% dplyr::select(-Date)) %>% 
  corrplot(method="circle",title="linear correlations")
```
Some obvious correlation in the three interest rates. There are also some strong correlations among the ETF bond flows that will require careful consideration. Is this due to overlapping categorisation?

## Information transfer measurement

The quantification of information transfer commonly relies on measures that have been derived from subject-specific assumptions and restrictions concerning the underlying stochastic processes or theoretical models.  With the development of transfer entropy, information theory based measures have become a popular alternative to quantify information flows within various disciplines.  Transfer entropy is a non-parametric measure of directed, asymmetric information transfer between two processes.
  
We quantify the information flow between two stationary time series and how to test for its statistical significance  using  Shannon transfer entropy within the package `RTransferEntropy`.A core aspect of the provided package is to allow statistical inference and hypothesis testing in the context of transfer entropy.

In the literature, the Granger causality has been addressed in many fields, including finance. Despite its success in the identification of couplings between the interacting variables, the use of structural models restricts its performance. Unlike Granger causality, TE is a quantity that is directly estimated from data and it does not suffer from such constraints. In the specific case of Gaussian distributed random variables, equivalence between TE and Granger causality has been proven.  
  
## Measuring information flows using transfer entropy
  
Let $log$ denote the logarithm to the base 2, then informational gain is measured in bits.
Shannon entropy [@S48] states that for a discrete random variable $J$ with probability distribution $p(j)$, where $j$ stands for the different outcomes the random variable $J$ can take, the average number of bits required to optimally encode independent draws from the distribution of $J$ can be calculated as 
  
$$
  H_J = - \sum_j p(j) \cdot log \left(p(j)\right).
$$
    
Strictly speaking, Shannon's formula  is a measure for uncertainty, which increases with the number of bits needed to optimally encode a sequence of realizations of $J$.
In order to measure the information flow between two processes, Shannon entropy is combined with the concept of the Kullback-Leibler distance [@KL51] and by assuming that the underlying processes evolve over time according to a Markov process [@schreiber2000].
Let $I$ and $J$ denote two discrete random variables with marginal probability distributions $p(i)$ and $p(j)$ and joint probability distribution $p(i,j)$, whose dynamical structures correspond to stationary Markov processes of order $k$ (process $I$) and $l$ (process $J$).
The Markov property implies that the probability to observe $I$ at time $t+1$ in state $i$ conditional on the $k$ previous observations is $p(i_{t+1}|i_t,...,i_{t-k+1})=p(i_{t+1}|i_t,...,i_{t-k})$.
The average number of bits needed to encode the observation in $t+1$ if the previous $k$ values are known is given by
  
$$
  h_I(k)=- \sum_i p\left(i_{t+1}, i_t^{(k)}\right) \cdot log \left(p\left(i_{t+1}|i_t^{(k)}\right)\right),
$$
where $i^{(k)}_t=(i_t,...,i_{t-k+1})$. $h_J(l)$ can be derived analogously for process $J$.
In the bivariate case, information flow from process $J$ to process $I$ is measured by quantifying the deviation from the generalized Markov property $p(i_{t+1}| i_t^{(k)})=p(i_{t+1}| i_t^{(k)},j_t^{(l)})$ relying on the Kullback-Leibler distance [@schreiber2000].
Thus, (Shannon) transfer entropy is given by 
  
$$
  T_{J \rightarrow I}(k,l) = \sum_{i,j} p\left(i_{t+1}, i_t^{(k)}, j_t^{(l)}\right) \cdot log \left(\frac{p\left(i_{t+1}| i_t^{(k)}, j_t^{(l)}\right)}{p\left(i_{t+1}|i_t^{(k)}\right)}\right),
$$
where $T_{J\rightarrow I}$ consequently measures the information flow from $J$ to $I$ ( $T_{I \rightarrow J}$ as a measure for the information flow from $I$ to $J$ can be derived analogously). 

Transfer entropy can also be based on RÃ©nyi entropy [@R70] rather than Shannon entropy.
RÃ©nyi entropy introduces a weighting parameter $q>0$ for the individual probabilities $p(j)$ and can be calculated as
$$
  H^q_J = \frac{1}{1-q} log \left(\sum_j p^q(j)\right).
$$
For $q\rightarrow 1$, RÃ©nyi entropy converges to Shannon entropy.
For $0<q<1$  events that have a low probability to occur receive more weight, while for $q>1$ the weights induce a preference for outcomes $j$ with a higher initial probability.
Consequently, RÃ©nyi entropy provides a more flexible tool for estimating uncertainty, since different areas of a distribution can be emphasized, depending on the parameter $q$. 

Using the escort distribution [for more information, see @BeckS93] $\phi_q(j)=\frac{p^q(j)}{\sum_j p^q(j)}$ with $q >0$ to normalize the weighted distributions, @JKS12 derive the RÃ©nyi transfer entropy measure as
$$
  RT_{J \rightarrow I}(k,l) = \frac{1}{1-q} log \left(\frac{\sum_i \phi_q\left(i_t^{(k)}\right)p^q\left(i_{t+1}|i^{(k)}_t\right)}{\sum_{i,j} \phi_q\left(i^{(k)}_t,j^{(l)}_t\right)p^q\left(i_{t+1}|i^{(k)}_t,j^{(l)}_t \right)}\right).
$$
Analogously to (Shannon) transfer entropy, RÃ©nyi transfer entropy measures the information flow from $J$ to $I$.
Note that, contrary to Shannon transfer entropy, the calculation of RÃ©nyi transfer entropy can result in negative values.
In such a situation, knowing the history of $J$ reveals even greater risk than would otherwise be indicated by only knowing the history of $I$ alone. For more details on this issue see @JKS12.
  
The above transfer entropy estimates are commonly biased due to small sample effects.
A remedy is provided by the effective transfer entropy [@MK02], which is computed in the following way: 
  
$$
  ET_{J \rightarrow I}(k,l)=  T_{J \rightarrow I}(k,l)- T_{J_{\text{shuffled}} \rightarrow I}(k,l),
$$
where $T_{J_{\text{shuffled}} \rightarrow I}(k,l)$ indicates the transfer entropy using a shuffled  version of the time series of $J$.
Shuffling implies randomly drawing values from the time series of $J$ and realigning them to generate a new time series.
This procedure destroys the time series dependencies of $J$ as well as the statistical dependencies between $J$ and $I$.
As a result $T_{J_{\text{shuffled}} \rightarrow I}(k,l)$ converges to zero with increasing sample size and any nonzero value of $T_{J_{\text{shuffled}} \rightarrow I}(k,l)$ is due to small sample effects.
The transfer entropy estimates from shuffled data can therefore be used as an estimator for the bias induced by these small sample effects.
To derive a consistent estimator, shuffling is repeated many times and the average of the resulting shuffled transfer entropy estimates across all replications is subtracted from the Shannon or RÃ©nyi transfer entropy estimate to obtain a bias corrected effective transfer entropy estimate.
  
In order to assess the statistical significance of transfer entropy estimates, we rely on  a Markov block bootstrap as proposed by @Dimpfl2013.
In contrast to shuffling, the Markov block bootstrap preserves the dependencies within each time series.
Thereby, it generates the distribution of transfer entropy estimates under the null hypothesis of no information transfer, i.e. randomly drawn blocks of process $J$ are realigned to form a simulated series, which retains the univariate dependencies of $J$ but eliminates the statistical dependencies between $J$ and $I$.
Shannon or RÃ©nyi transfer entropy is then estimated based on the simulated time series.
Repeating this procedure yields the distribution of the transfer entropy estimate under the null of no information flow.
The p-value associated with the null hypothesis of no information transfer is given by $1-\hat{q}_{TE}$, where $\hat{q}_{TE}$ denotes the quantile of the simulated distribution that corresponds to the original transfer entropy estimate.
  
The calculation of Shannon and RÃ©nyi transfer entropy is based on discrete data.
If the data does not exhibit a discrete structure that allows for transfer entropy estimation, it has to be discretized.
This can be achieved by symbolic recoding, i.e. by partitioning the data into a finite number of bins, which can either be based on defining upper and lower bounds for the bins a priori or by choosing specific quantiles of the empirical distribution of the data.
Denote the bounds specified for the $n$ bins by $q_1, q_2, ..., q_n$, where $q_1< q_2< ... <q_n$, and consider a time series  denoted by $y_t$,  the data is recoded as

$$
S_t=
\begin{cases}
~1~~~~~~~~ \mbox{ for }~  y_t\leq q_1\\
~ 2~ ~~~~~~~\mbox{ for }~  q_1<y_t\leq q_2\\
~\vdots~~~~~~~~~~~~~~~~~\vdots\\
~n-1~~\mbox{ for }~  q_{n-1}<y_t \leq q_n\\
~n ~~~~~~~~\mbox{     for } ~ y_t\geq q_n
\end{cases}.
$$
Thereby, each value in the observed time series $y_t$ is replaced by an integer ($1$,$2$,...,$n$), according to how $S_t$ relates to the interval specified by the lower and upper bounds $q_1$ to $q_n$.
The choice of the bins should be motivated by the distribution of the data.
However, we recommend that the number of bins is limited in order to avoid too many zero observations when calculating relative frequencies as estimators of the joint probabilities in the (effective) transfer entropy equations.

Shannon's transfer entropy measure the information flow (reduction in uncertainty in probability) is a model-free way that captures the non-linear complexity of a financial network.  As an exploratory exercise, we assess the information transfer from and to the changes of three interest rates. 

```{r EU, fig.cap='Effective Transfer Entropy for Change EU Libor'}
load("eteResults.RData")
rename_data<-function(df){
 df[, ticker := factor(ticker, 
                        levels = unique(df$ticker)[order(df[dir == "X->Y"]$ete)])]
  df[, dir := factor(dir, levels = c("X->Y", "Y->X"),
                          labels = c("Flow towards Interest Rate Changes",
                                     "Flow towards ETF"))]
}
plot_data<-rename_data(df_EU)
ggplot(plot_data, aes(x = ticker, y = ete)) + 
  facet_wrap(~dir) +
  geom_hline(yintercept = 0, color = "gray") +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = NULL, y = "Effective Transfer Entropy") +
  geom_errorbar(aes(ymin = ete - qnorm(0.95) * se,  
                    ymax = ete + qnorm(0.95) * se),  
                width = 0.25, col = "blue") +
  geom_point()
```

```{r US,fig.cap='Effective Transfer Entropy for Change US Libor'}
plot_data<-rename_data(df_US)
ggplot(plot_data, aes(x = ticker, y = ete)) + 
  facet_wrap(~dir) +
  geom_hline(yintercept = 0, color = "gray") +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = NULL, y = "Effective Transfer Entropy") +
  geom_errorbar(aes(ymin = ete - qnorm(0.95) * se,  
                    ymax = ete + qnorm(0.95) * se),  
                width = 0.25, col = "blue") +
  geom_point()
```

```{r}
plot_data<-rename_data(df_SONIA)
ggplot(plot_data, aes(x = ticker, y = ete)) + 
  facet_wrap(~dir) +
  geom_hline(yintercept = 0, color = "gray") +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = NULL, y = "Effective Transfer Entropy") +
  geom_errorbar(aes(ymin = ete - qnorm(0.95) * se,  
                    ymax = ete + qnorm(0.95) * se),  
                width = 0.25, col = "blue") +
  geom_point()
```



# Linear VAR models with interactions

Vector autoregressive models are a conventional *go to* to analysis dynamic relationships in finance, predicated on the assumption of linearity and spherical error disturbances. What follows is an analysis of all the ETF flows and daily $\delta$ in each interest rate.  
 

```{r}
library(vars)
var_model<-VAR(y_endog_scaled %>% 
                dplyr::select(-CE_dummy,-Date),
               exogen =y_endog_scaled %>% 
                 mutate(Corporate_i=`Corporate Bonds`*CE_dummy,
                        HighY_i=`High Yield`*CE_dummy) %>% 
                 dplyr::select(Corporate_i,HighY_i,CE_dummy),
               lag.max = 4)
var_model %>% summary() -> var_model_results
```
### Daily $\delta$ US VAR equation

```{r US results}
var_model_results$varresult$d_US
```

### Daily $\delta$ SONIA VAR equation

```{r Sonia results}
var_model_results$varresult$d_SONIA
```

### EU Libor Rate Change VAR equation
```{r EU Results}
var_model_results$varresult$d_EU
```
### impulse response analysis

```{r irf run}
irf_results<-irf(var_model,n.ahead = 20,runs = 500,seed = 123)
```


```{r irf}
bind_cols(
irf_results$Lower$d_US %>% 
  as.tibble() %>%
  mutate(steps=1:n()) %>%
  gather(x,lower,-steps),
  
irf_results$irf$d_US %>% 
  as.tibble() %>%
  gather(x,value) %>%
  dplyr::select(value),

irf_results$Upper$d_US %>% 
  as.tibble() %>%
  gather(x,upper) %>%
  dplyr::select(upper)
)->d_US_irf

d_US_irf %>% 
  filter(sign(lower)==sign(upper),value!=0)  %>%
ggplot(aes(x=steps,y=value,ymin=lower,ymax=upper))+ geom_point() +
  geom_errorbar() + facet_wrap(~x) +
  labs(title="Impulse Response for Change in US Libor",subtitle = "95% bootstrapped intervals (n=500)")
```

```{r EUimpulse}
bind_cols(
irf_results$Lower$d_EU %>% 
  as.tibble() %>%
  mutate(steps=1:n()) %>%
  gather(x,lower,-steps),
  
irf_results$irf$d_EU %>% 
  as.tibble() %>%
  gather(x,value) %>%
  dplyr::select(value),

irf_results$Upper$d_EU %>% 
  as.tibble() %>%
  gather(x,upper) %>%
  dplyr::select(upper)
)->d_irf

d_irf %>% 
  filter(sign(lower)==sign(upper)& value!=0 & lower!=0 & upper!=0)  %>%
ggplot(aes(x=steps,y=value,ymin=lower,ymax=upper))+ geom_point() +
  geom_errorbar() + facet_wrap(~x) +
  labs(title="Impulse Response for Change in EU Libor",subtitle = "95% bootstrapped intervals(n=500)")
```

```{r Sonia}
bind_cols(
irf_results$Lower$d_SONIA %>% 
  as.tibble() %>%
  mutate(steps=1:n()) %>%
  gather(x,lower,-steps),
  
irf_results$irf$d_SONIA %>% 
  as.tibble() %>%
  gather(x,value) %>%
  dplyr::select(value),

irf_results$Upper$d_SONIA %>% 
  as.tibble() %>%
  gather(x,upper) %>%
  dplyr::select(upper)
)->d_irf

d_irf %>% 
  filter(sign(lower)==sign(upper) & value!=0 & lower!=0 & upper!=0)  %>%
ggplot(aes(x=steps,y=value,ymin=lower,ymax=upper))+ geom_point() +
  geom_errorbar() + facet_wrap(~x) +
  labs(title="Impulse Response for Change in SONIA",subtitle = "95% bootstrapped intervals(n=500)")
```


# Reference

Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions. Proceedings of the 31st International Conference on Neural Information Processing Systems, 4768–4777.

Brandt, P. T., Colaresi, M., & Freeman, J. R. (2008). The Dynamics of Reciprocity, Accountability, and Credibility. The Journal of Conflict Resolution, 52(3), 343–374.

Brandt, P. T., & Freeman, J. R. (2006). Advances in Bayesian Time Series Modeling and the Study of Politics: Theory Testing, Forecasting, and Policy Analysis. Political Analysis: An Annual Publication of the Methodology Section of the American Political Science Association, 14(1), 1–36.

Brandt, P. T., Freeman, J. R., & Schrodt, P. A. (2014). Evaluating forecasts of political conflict dynamics. International Journal of Forecasting, 30(4), 944–962.

Beck, Christian, and Friedrich Schögl. 1993. Thermodynamics of Chaotic Systems: An Introduction. Cambridge Nonlinear Science Series 4. Cambridge University Press.

Dimpfl, Thomas, and Franziska Julia Peter. 2013. “Using Transfer Entropy to Measure Information Flows Between Financial Markets.” Studies in Nonlinear Dynamics and Econometrics 17 (1): 85–102.

Jizba, Petr, Hagen Kleinert, and Mohammad Shefaat. 2012. “Renyi’s Information Transfer Between Financial Time Series.” Physica A 391: 2971–89.

Kullback, S., and R. A. Leibler. 1951. “On Information and Sufficiency.” The Annals of Mathematical Statistics 1: 79–86.

Marschinski, R., and H. Kantz. 2002. “Analysing the Information Flow Between Financial Time Series: An Improved Estimator for Transfer Entropy.” European Physical Journal B 30 (2): 275–81.

Rényi, A. 1970. Probability Theory. North-Holland, Amsterdam.
Schreiber, T. 2000. “Measuring Information Transfer.” Physical Review Letters 85 (2): 461–64.

Shannon, C. E. 1948. “A Mathematical Theory of Communication.” Bell System Technical Journal 27: 379–423.

Sims, C. A., Waggoner, D. F., & Zha, T. (2008). Methods for inference in large multiple-equation Markov-switching models. Journal of Econometrics, 146(2), 255–274.

Sims, C. A., & Zha, T. (1998). Bayesian Methods for Dynamic Multivariate Models. International Economic Review, 39(4), 949–968.

Sims, C. A., & Zha, T. (2006). Were There Regime Switches in U.S. Monetary Policy? The American Economic Review, 96(1), 54–81.

